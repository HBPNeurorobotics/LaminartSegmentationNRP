<?xml version="1.0" ?>
<ns1:bibi xmlns:ns1="http://schemas.humanbrainproject.eu/SP10/2014/BIBI" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">


    <ns1:brainModel> <!-- "brain"??? -->
        <ns1:file>LaminartBrainInNRP.py</ns1:file>
    </ns1:brainModel>


    <ns1:bodyModel>model.sdf</ns1:bodyModel>


    <ns1:transferFunction src="csv_spike_monitor.py" xsi:type="ns1:PythonTransferFunction"/>


    <ns1:transferFunction xsi:type="ns1:PythonTransferFunction">
    #
    @nrp.NeuronMonitor(nrp.brain.record, nrp.spike_recorder)
    def all_neurons_monitor(t):
        return True
    #
    </ns1:transferFunction> <!-- TF to set things to record neurons??? -->


    <ns1:transferFunction xsi:type="ns1:PythonTransferFunction">
    #
    from sensor_msgs.msg import JointState

    @nrp.MapVariable(&quot;eye_position&quot;, initial_value=None, scope=nrp.GLOBAL)
    @nrp.MapRobotSubscriber(&quot;joints&quot;, Topic(&quot;/robot/joints&quot;, JointState))
    @nrp.Robot2Neuron()
    def set_eyepos(t, eye_position, joints):
        joints = joints.value
        eye_position.value = joints.position[joints.name.index('eye_version')]
    #
    </ns1:transferFunction> <!-- TF to set the eyes position IMPORTANT POUR MOI -->


    <ns1:transferFunction xsi:type="ns1:PythonTransferFunction">
    #
    <!-- Input transfer function that transforms the camera input into a DC current source fed to LGN neurons -->
    import numpy, scipy.ndimage.interpolation, sensor_msgs.msg
    from cv_bridge import CvBridge

    @nrp.MapRobotSubscriber(&quot;camera&quot;, Topic(&quot;/icub_model/left_eye_camera/image_raw&quot;, sensor_msgs.msg.Image))
    @nrp.MapSpikeSource(&quot;LGNBrightInput&quot;, nrp.map_neurons(range(0, ImageNumPixelRows*ImageNumPixelColumns), lambda i: nrp.brain.LGNBright[i]), nrp.dc_source)
    @nrp.MapSpikeSource(&quot;LGNDarkInput&quot;,   nrp.map_neurons(range(0, ImageNumPixelRows*ImageNumPixelColumns), lambda i: nrp.brain.LGNDark[i]),   nrp.dc_source)
    @nrp.Robot2Neuron()
    def grab_image(t, camera, LGNBrightInput, LGNDarkInput):

        image = camera.value
        if image is not None:

            <!-- Read the image into an array, mean over 3 colors, resize it to the dimensions of the network and flatten the result -->
            imgIn = numpy.mean(CvBridge().imgmsg_to_cv2(image, "rgb8"), axis=2)
            resizeFactor = (float(ImageNumPixelRows)/imgIn.shape[0], float(ImageNumPixelColumns)/imgIn.shape[1])
            imgResized = scipy.ndimage.interpolation.zoom(imgIn, resizeFactor, order=3).flatten()

            <!-- Give the pre-processed image to the LGN (bright and dark inputs) -->
            LGNBrightInput.amplitude = numpy.random.poisson(10.0*max(0.0, (imgIn/127.0-1.0)))
            LGNDarkInput  .amplitude = numpy.random.poisson(10.0*max(0.0, 1.0-(imgIn/127.0)))
    #
    </ns1:transferFunction>




    <ns1:transferFunction xsi:type="ns1:PythonTransferFunction">
    #
    <!-- Output transfer function that transforms the Laminart network output into a movement of the robot's eyes (for now: whatever movement) -->
    from std_msgs.msg import Float64
    <!-- THE NEXT LINE INITIALIZE A GLOBAL VARIABLE NECESSARY FOR THE OUTPUT TRANSFER FUNCTION (declare here = ok?) -->
    V4SpikeCountUpToLastStep = numpy.array([[[0 for j in range(ImageNumPixelColumns)] for i in range(ImageNumPixelRows)] for h in range(numSegmentationLayers)])

    @nrp.MapVariable(&quot;eyePosition&quot;, scope=nrp.GLOBAL)
    @nrp.MapSpikeSink(&quot;V4&quot;, nrp.brain.V4Brightness, nrp.leaky_integrator_alpha)
    @nrp.Neuron2Robot(Topic(&quot;/robot/eye_version/pos&quot;, Float64))
    def tf_results(t, eyePosition, V4):

        <!-- Record the activity of the V4 brightness segmentation layers and store the last step spike count in a numpy array -->
        V4SpikeCountUpToNow  = [value for (key, value) in sorted(V4.get_spike_counts().items())]
        V4SpikeCountThisStep = numpy.array([[[0 for j in range(ImageNumPixelColumns)] for i in range(ImageNumPixelRows)] for h in range(numSegmentationLayers)])
        for h in range(nrp.numSegmentationLayers):
            for i in range(nrp.ImageNumPixelRows):
                for j in range(nrp.ImageNumPixelColumns):
                    V4SpikeCountThisStep[h][i][j] += V4SpikeCountUpToNow[h*ImageNumPixelRows*ImageNumPixelColumns + i*ImageNumPixelColumns + j] - V4SpikeCountUpToLastStep[h][i][j]
                    V4SpikeCountUpToLastStep[h][i][j] += V4SpikeCountThisStep[h][i][j]  # update cumulative spikes

        <!-- Take segmentation layer with the most activity and find the position of with the most contrast within (DUMB CRITERION) -->
        <!-- A CRITERION COULD BE E.G. A TASK-RELATED (LIKE IN FACE DETECTOR) SALIENCY MAP MAXIMUM, BETWEEN ALL SEGMENTATION LAYERS -->
        winnerSegmentationLayer = numpy.argmax(numpy.sum(V4SpikeCountThisStep, axis=(1,2)))
        winnerSquaredGradientMap = numpy.sum(numpy.square(numpy.gradient(V4SpikeCountThisStep[winnerSegmentationLayer])), axis=0) <!-- sum both gradient directions ("sum(..., axis=0)") -->
        targetPosition = list(numpy.unravel_index(numpy.argmax(winnerSquaredGradientMap), (ImageNumPixelRows, ImageNumPixelColumns)))
        displacementVector = ImageNumPixelRows/2 - targetPosition[0]

        <!-- New eye position (reduced and smoothed to the maximum possible movement) -->
        maxMov = 0.25 <!-- in rad, corresponds to 15 degrees -->
        return eyePosition.value + maxMov*numpy.tanh(displacementVector) <!-- new eye position -->
    #
    </ns1:transferFunction>



    <ns1:transferFunction xsi:type="ns1:PythonTransferFunction">
    #
    <!-- Input transfer function that sends segmentation signals, following top-down and bottom-up cues (dumb choice of position for now) -->
    import random

    @nrp.MapRobotSubscriber(&quot;camera&quot;, Topic(&quot;/icub_model/left_eye_camera/image_raw&quot;, sensor_msgs.msg.Image))
    @nrp.MapSpikeSource(&quot;SurfaceSegmentationOffSignal&quot;, nrp.map_neurons(range(0, (numSegmentationLayers-1)*ImageNumPixelRows*ImageNumPixelColumns), lambda i: SurfaceSegmentationOff[i]), nrp.dc_source)
    @nrp.MapSpikeSource(&quot;SurfaceSegmentationOnSignal&quot;,  nrp.map_neurons(range(0, (numSegmentationLayers-1)*ImageNumPixelRows*ImageNumPixelColumns), lambda i: SurfaceSegmentationOn [i]), nrp.dc_source)
    @nrp.MapSpikeSource(&quot;BoundarySegmentationSignal&quot;,   nrp.map_neurons(range(0, (numSegmentationLayers-1)*ImageNumPixelRows*ImageNumPixelColumns), lambda i: BoundarySegmentationOn[i]), nrp.dc_source)
    @nrp.Robot2Neuron()
    def send_segmentation_signals(t, camera, SurfaceSegmentationOffSignal, SurfaceSegmentationOnSignal, BoundarySegmentationSignal):

        image = camera.value
        if image is not None:

            <!-- Read the image into an array, mean over 3 colors and resize it to the dimensions of the network -->
            imgIn = numpy.mean(CvBridge().imgmsg_to_cv2(image, "rgb8"), axis=2)
            resizeFactor = (float(ImageNumPixelRows)/imgIn.shape[0], float(ImageNumPixelColumns)/imgIn.shape[1])
            imgResized = scipy.ndimage.interpolation.zoom(imgIn, resizeFactor, order=3)

            <!-- Loop through all non-basic segmentation layers and send signals around top-down / bottom-up selected targets -->
            surfaceOnTarget = []
            surfaceOffTarget = []
            boundaryOnTarget = []
            for h in range(numSegmentationLayers-1):

                <!-- Look for the best place to send a segmentation signal (DUMB CRITERION) -->
                <!-- HERE WE SHOULD DEVELOP THE CRITERION (TASK-RELATED SALIENCY MAX) AND ALSO IMPLEMENT AND TASK RELATED TOP-DOWN SIGNALS -->
                squaredGradientMap = numpy.sum(numpy.square(numpy.gradient(imgResized)), axis=0) # sum both gradient directions ("sum(..., axis=0)")
                targetPosition = list(numpy.unravel_index(numpy.argmax(squaredGradientMap), (ImageNumPixelRows, ImageNumPixelColumns)))
                if useSDPropToDist:
                    segmentationTargetLocationSD = minSD + rateSD*numpy.sqrt((ImageNumPixelRows - targetPosition[0])**2 + (ImageNumPixelColumns - targetPosition[1])**2)
                segmentLocationRow = int(round(random.gauss(targetPosition[0], segmentationTargetLocationSD)))
                segmentLocationCol = int(round(random.gauss(targetPosition[1], segmentationTargetLocationSD)))

                <!-- Define surface segmentation signals (gives local DC inputs to surface and boundary segmentation networks) -->
                if useSurfaceSegmentation:
                    for i in range(0, ImageNumPixelRows):         <!-- Rows -->
                        for j in range(0, ImageNumPixelColumns):  <!-- Columns -->

                            <!-- Off signals are at borders of image (will flow in to other locations unless stopped by boundaries) -->
                            if i==0 or i==(ImageNumPixelRows-1) or j==0 or j==(ImageNumPixelColumns-1):
                                surfaceOffTarget.append(h*ImageNumPixelRows*ImageNumPixelColumns + i*ImageNumPixelColumns + j)

                            <!-- On signals start the surface segmentation at a specific location and flow across closed shapes formed by boundaries -->
                            distance = numpy.sqrt(numpy.power(segmentLocationRow-i, 2) + numpy.power(segmentLocationCol-j, 2))
                            if distance &gt; segmentationSignalSize:
                                surfaceOnTarget.append(h*ImageNumPixelRows*ImageNumPixelColumns + i*ImageNumPixelColumns + j)

                <!-- Define boundary segmentation signals -->
                if useBoundarySegmentation:
                    for i in range(0, ImageNumPixelRows):         <!-- Rows -->
                        for j in range(0, ImageNumPixelColumns):  <!-- Columns -->

                            <!-- On signals start the boundary segmentation at a specific location and flow along connected boundaries -->
                            distance = numpy.sqrt(numpy.power(segmentLocationRow-i, 2) + numpy.power(segmentLocationCol-j, 2))
                            if distance &gt; segmentationSignalSize:
                                boundaryOnTarget.append(h*ImageNumPixelRows*ImageNumPixelColumns + i*ImageNumPixelColumns + j)

            <!-- Set a firing positive firing rate for concerned units of the segmentation top-down signal -->
            if len(surfaceOffTarget) > 0:
                SurfaceSegmentationOffSignal[surfaceOffTarget].amplitude = 1.0
            if len(surfaceOnTarget)  > 0:
                SurfaceSegmentationOnSignal [surfaceOnTarget] .amplitude = 1.0
            if len(boundaryOnTarget) > 0:
                BoundarySegmentationSignal  [boundaryOnTarget].amplitude = 1.0
    #
    </ns1:transferFunction>


    <ns1:transferFunction xsi:type="ns1:PythonTransferFunction">
    #
    from sensor_msgs.msg import JointState

    @nrp.MapRobotSubscriber(&quot;joints&quot;, Topic(&quot;/robot/joints&quot;, JointState))
    @nrp.Neuron2Robot(Topic('/joint_states', JointState))
    def filter_joints_for_nice_output_on_frontend(t, joints):
        from sensor_msgs.msg import JointState

        joints = joints.value
        to_forward = ['eye_version']

        ret = JointState()
        ret.header = joints.header
        ret.name = to_forward
        ret.position = [joints.position[joints.name.index(x)] for x in to_forward]
        ret.velocity = [joints.velocity[joints.name.index(x)] for x in to_forward]
        ret.effort = [joints.effort[joints.name.index(x)] for x in to_forward]

        return ret
    #
    </ns1:transferFunction>


</ns1:bibi>